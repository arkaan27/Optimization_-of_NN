# Optimization_of_NN

Performs Batch gradient descent with huge data set. Initializes velocity and updates parameters with Momentum 
Uses Adam Optimizer to further improve performance.

# Gradient Checking

Uses Gradient Checking for N dimentional in Forward Propagation and Backward Propagation

# Initialization

Initializes parameters randomly, with zeros and with He.

# Regularization

Performs Regularization with Backward  Propagation, computes cost.
Performs Drop out Regularization on Backward Propagation and Forward propagation
